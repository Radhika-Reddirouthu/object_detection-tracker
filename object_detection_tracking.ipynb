{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as vis_util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The function below takes the input from the user and create the tracker if required as per user's choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_for_input():\n",
    "    print(\"1. object detection\")\n",
    "    print(\"2. object tracting\")\n",
    "    print(\"3. both object detection and tracking\")\n",
    "    choice=input(\"select any of the beow options\")\n",
    "    if choice==\"2\" or choice==\"3\":\n",
    "        print(\"1. boosting\")\n",
    "        print(\"2. MIL\")\n",
    "        print(\"3. KCF\")\n",
    "        print(\"4. TLD\")\n",
    "        print(\"5. MEDIANFLOW\")\n",
    "        print(\"6. GOTURN\")\n",
    "        print(\"7. MOSSE\")\n",
    "        print(\"8. CSRT\")\n",
    "        tracker_name=input(\"please select your tracker\")\n",
    "        if choice=='1':\n",
    "            tracker=cv2.TrackerBoosting_create()\n",
    "        elif choice=='2':\n",
    "            tracker=cv2.TrackerMIL_create()\n",
    "        elif choice=='3':\n",
    "            tracker=cv2.TrackerKCF_create()\n",
    "        elif choice=='4':\n",
    "            tracker=cv2.TrackerTLD_create()\n",
    "        elif choice=='5':\n",
    "            tracker=cv2.TrackerMedianFlow_create()\n",
    "        elif choice=='6':\n",
    "            tracker=cv2.TrackerGOTURN_create()\n",
    "        elif choice=='7':\n",
    "            tracker=cv2.TrackerMOSSE_create()\n",
    "        elif choice=='8':\n",
    "            tracker=cv2.TrackerCSRT_create()\n",
    "        else:\n",
    "            tracker=cv2.TrackerBoosting_create()   \n",
    "    else:\n",
    "        tracker=\"0\"\n",
    "    return choice,tracker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The below function is called when the user needs only \"object detection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def object_detection():\n",
    "    if tf.__version__ < '1.10.0':\n",
    "        raise ImportError('Please upgrade your tensorflow installation to v1.10.* or later!')\n",
    "    PATH_TO_CKPT = './frozen_inference_graph.pb'\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    detection_graph = tf.Graph()\n",
    "    with detection_graph.as_default():\n",
    "        od_graph_def = tf.GraphDef()\n",
    "        with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
    "            serialized_graph = fid.read()\n",
    "            od_graph_def.ParseFromString(serialized_graph)\n",
    "            tf.import_graph_def(od_graph_def, name='')\n",
    "    THRESHOLD = 0.6  #  threshold for detections i.e.,we are considering the objects only if the score of the object is above 60% \n",
    "    PATH_TO_LABELS = os.path.join('mscoco_label_map.pbtxt')\n",
    "    NUM_CLASSES = 90\n",
    "    label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
    "    categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
    "    category_index = label_map_util.create_category_index(categories)\n",
    "    with detection_graph.as_default():\n",
    "        with tf.Session(graph=detection_graph) as sess:\n",
    "            #provide the ip address that is shown in your mobile when you start the video stream in the app\n",
    "            cap = cv2.VideoCapture(\"http://192.168.1.6:8080/video\")\n",
    "            #comment the above line and uncomment the below line if web cam is used \n",
    "            #cap=cv2.VideoCapture(0)\n",
    "            # Check if the app in the mobile is opened correctly and mobile is connected to same wifi as of your machine\n",
    "            if not cap.isOpened():\n",
    "                raise IOError(\"Cannot connect\")\n",
    "            # Definite input and output Tensors for detection_graph\n",
    "            image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "            # Each box in the image represents a particular object that is detected.\n",
    "            detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "            # Score represents percentage of confidence that model have for each of the objects.\n",
    "            # Images only above 60% score are shown to the user\n",
    "            # score value is shown on the result image, together with the class label.\n",
    "            detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "            detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "            num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
    "            ret, frame = cap.read()\n",
    "            height, width, _ = frame.shape\n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                (boxes, scores, classes, num) = sess.run(\n",
    "                  [detection_boxes, detection_scores, detection_classes, num_detections],\n",
    "                  feed_dict={image_tensor: np.expand_dims(frame, axis=0)})\n",
    "                for i, box in enumerate(boxes[0]):\n",
    "                    if scores[0][i] > THRESHOLD:\n",
    "                        x = box[1] * width\n",
    "                        y = box[0] * height\n",
    "                        right = box[3] * width\n",
    "                        bottom = box[2] * height\n",
    "                        frame = cv2.rectangle(frame,(int(x),int(y)),(int(right),int(bottom)),(0,0,255),2)\n",
    "                        vis_util.visualize_boxes_and_labels_on_image_array(frame,np.squeeze(boxes),np.squeeze(classes).astype(np.int32),np.squeeze(scores),category_index,use_normalized_coordinates=True,line_thickness=8)\n",
    "\n",
    "                cv2.imshow('object_detection', frame)\n",
    "\n",
    "                # Press esc to exit\n",
    "                if cv2.waitKey(50)&0xFF==27:\n",
    "                    break\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The below function is called when the user needs object tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def object_tracking(tracker):\n",
    "    tracker_name=str(tracker).split()[0][1:]\n",
    "    #provide the ip address that is shown in your mobile when you start the video stream in the app\n",
    "    cap = cv2.VideoCapture(\"http://192.168.1.6:8080/video\")\n",
    "    #comment the above line and uncomment the below line if web cam is used \n",
    "    #cap=cv2.VideoCapture(0)\n",
    "    ret,frame=cap.read() #read the first frame\n",
    "    roi=cv2.selectROI(frame,False) #select ROI\n",
    "    ret=tracker.init(frame,roi) #initializing tracker\n",
    "    mask=np.zeros_like(frame)\n",
    "    color=(0,255,0)\n",
    "    temp=(0,0)\n",
    "    while True:\n",
    "        ret,frame=cap.read() \n",
    "        success,roi=tracker.update(frame) #updating the tracker\n",
    "        (x,y,w,h)=tuple(map(int,roi)) \n",
    "        #drawing rectangle as tracker moves\n",
    "        if success:\n",
    "            pts1=(x,y)\n",
    "            pts2=(x+w,y+h)\n",
    "            cv2.rectangle(frame,pts1,pts2,(255,125,25),3)\n",
    "            mask=cv2.line(mask,pts1,temp,color,2)\n",
    "            frame=cv2.circle(frame,temp,3,color,-1)\n",
    "            frame=cv2.add(frame,mask)\n",
    "            temp=pts1\n",
    "        else:\n",
    "            cv2.putText(frame,\"fail to track the object\",(100,200),cv2.FONT_HERSHEY_SIMPLEX,1,(25,125,255),3)\n",
    "        cv2.putText(frame,tracker_name,(20,400),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,0),3)\n",
    "        cv2.imshow(\"object_tracking\",frame)\n",
    "        if cv2.waitKey(50)&0xFF==27:\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The below function is called when the user needs both object detection and tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def object_detection_tracking(tracker):\n",
    "    tracker_name=str(tracker).split()[0][1:] \n",
    "    if tf.__version__ < '1.10.0':\n",
    "        raise ImportError('Please upgrade your tensorflow installation to v1.10.* or later!')\n",
    "    PATH_TO_CKPT = './frozen_inference_graph.pb'\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    detection_graph = tf.Graph()\n",
    "    with detection_graph.as_default():\n",
    "        od_graph_def = tf.GraphDef()\n",
    "        with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
    "            serialized_graph = fid.read()\n",
    "            od_graph_def.ParseFromString(serialized_graph)\n",
    "            tf.import_graph_def(od_graph_def, name='')\n",
    "    THRESHOLD = 0.6  #threshold for detections i.e., we are considering the objects only if the score of the object is above 60% \n",
    "    PATH_TO_LABELS = os.path.join('mscoco_label_map.pbtxt')\n",
    "    NUM_CLASSES = 90\n",
    "    label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
    "    categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
    "    category_index = label_map_util.create_category_index(categories)\n",
    "    with detection_graph.as_default():\n",
    "        with tf.Session(graph=detection_graph) as sess:\n",
    "            #provide the ip address that is shown in your mobile when you start the video stream in the app\n",
    "            cap = cv2.VideoCapture(\"http://192.168.1.6:8080/video\")\n",
    "            #comment the above line and uncomment the below line if web cam is used \n",
    "            #cap=cv2.VideoCapture(0)\n",
    "            # Check if the app in the mobile is opened correctly and mobile is connected to same wifi as of your machine\n",
    "            if not cap.isOpened():\n",
    "                raise IOError(\"Cannot connect\")\n",
    "            # Definite input and output Tensors for detection_graph\n",
    "            image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "            # Each box in the image represents a particular object that is detected.\n",
    "            detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "            # Score represents percentage of confidence that model have for each of the objects.\n",
    "            # Images only above 60% score are shown to the user\n",
    "            # score value is shown on the result image, together with the class label.\n",
    "            detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "            detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "            num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
    "            ret, frame = cap.read() \n",
    "            # frame is stored in temp_frame for further use in object tracking part as there are changes done to frame while object detection \n",
    "            temp_frame=frame \n",
    "            roi=cv2.selectROI(temp_frame,False) #select ROI\n",
    "            ret=tracker.init(temp_frame,roi) #initializing tracker\n",
    "            mask=np.zeros_like(temp_frame)\n",
    "            color=(0,255,0)\n",
    "            temp=(0,0)\n",
    "            height, width, _ = frame.shape\n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                temp_frame=frame\n",
    "                (boxes, scores, classes, num) = sess.run(\n",
    "                  [detection_boxes, detection_scores, detection_classes, num_detections],\n",
    "                  feed_dict={image_tensor: np.expand_dims(frame, axis=0)})\n",
    "                for i, box in enumerate(boxes[0]):\n",
    "                    if scores[0][i] > THRESHOLD:\n",
    "                        x = box[1] * width\n",
    "                        y = box[0] * height\n",
    "                        right = box[3] * width\n",
    "                        bottom = box[2] * height\n",
    "                        frame = cv2.rectangle(frame,(int(x),int(y)),(int(right),int(bottom)),(0,0,255),2)\n",
    "                        vis_util.visualize_boxes_and_labels_on_image_array(frame,np.squeeze(boxes),np.squeeze(classes).astype(np.int32),np.squeeze(scores),category_index,use_normalized_coordinates=True,line_thickness=8)\n",
    "                ### End of Object detection###\n",
    "                ### Start of object tracking###\n",
    "                success,roi=tracker.update(temp_frame) #updating tracker\n",
    "                (x,y,w,h)=tuple(map(int,roi))\n",
    "                #Draw rectangle as object moves\n",
    "                if success:\n",
    "                    pts1=(x,y)\n",
    "                    pts2=(x+w,y+h)\n",
    "                    cv2.rectangle(frame,pts1,pts2,(255,125,25),3)\n",
    "                    mask=cv2.line(mask,pts1,temp,color,2)\n",
    "                    frame=cv2.circle(frame,temp,3,color,-1)\n",
    "                    frame=cv2.add(frame,mask)\n",
    "                    temp=pts1\n",
    "                else:\n",
    "                    cv2.putText(frame,\"fail to track the object\",(100,200),cv2.FONT_HERSHEY_SIMPLEX,1,(25,125,255),3)\n",
    "                cv2.putText(frame,tracker_name,(20,400),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,0),3)\n",
    "\n",
    "                cv2.imshow('object_detection_tracking', frame)\n",
    "\n",
    "                # Press esc to exit\n",
    "                if cv2.waitKey(50)&0xFF==27:\n",
    "                    break\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the below code to start the object detection or tracking or both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. object detection\n",
      "2. object tracting\n",
      "3. both object detection and tracking\n",
      "select any of the beow options3\n",
      "1. boosting\n",
      "2. MIL\n",
      "3. KCF\n",
      "4. TLD\n",
      "5. MEDIANFLOW\n",
      "6. GOTURN\n",
      "7. MOSSE\n",
      "8. CSRT\n",
      "please select your tracker1\n"
     ]
    }
   ],
   "source": [
    "choice, tracker=ask_for_input()\n",
    "if choice==\"1\":\n",
    "    object_detection()\n",
    "elif choice==\"2\":\n",
    "    object_tracking(tracker)\n",
    "elif choice==\"3\":\n",
    "    object_detection_tracking(tracker)\n",
    "else:\n",
    "    print(\"enter value inputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
